{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Classification with Skit-Learn\n",
    "\n",
    "<br>\n",
    "This exercise is based on the code and prompt created by <a href=\"https://www.youtube.com/watch?v=T5pRlIbr6gg&list=PL2-dafEMk2A6QKz1mrk1uIGfHkC1zZ6UU\" target="_blank"> Siraj Raval </a>. \n",
    "\n",
    "## Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree  # Going to create a decision tree \n",
    "\n",
    "# Create a list [height, weight, shoe size]\n",
    "\n",
    "X = [[181, 80, 44], [177, 70, 43], [160, 60, 38], [154, 54, 37], [166, 65, 40],\n",
    "     [190, 90, 47], [175, 64, 39],\n",
    "     [177, 70, 40], [159, 55, 37], [171, 75, 42], [181, 85, 43]]\n",
    "\n",
    "# The values in the list correspond with the above body measurements \n",
    "\n",
    "Y = ['male', 'male', 'female', 'female', 'male', 'male', 'female', 'female',\n",
    "     'female', 'male', 'male']\n",
    "\n",
    "# Let's create our classifier/model \n",
    "\n",
    "model = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Let's train our dataset using the variable we previously created\n",
    "# The fit method trains our decision tree on our dataset\n",
    "\n",
    "model = model.fit(X,Y)\n",
    "\n",
    "# Let's test our model by trying to classify a new set of body measurements\n",
    "# Let's store the result in a variable called \"prediction\"\n",
    "\n",
    "prediction = model.predict([[190, 70, 43]])\n",
    "\n",
    "print(prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Find 3 more classifiers from the sci-kit learn documentation and train them on the same dataset and compare their results. You can determine accuracy by trying to predict your trained classifier on samples from the training data and see if it correctly classifies it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "A classification alogirithm for binary output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "\n",
    "log_model.fit(X,Y)\n",
    "\n",
    "log_prediction = log_model.predict([[190, 70, 43]])\n",
    "\n",
    "print(log_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Naive Bayes \n",
    "\n",
    "<strong> Naive Bayes </strong> is a collection of classification algorithms based on <strong> Bayes Theorem </strong>. It is not a single algorithm but a family of algorithms that all share a common principle, that every feature being classified is independent of the value of any other feature.\n",
    "\n",
    "For example, a fruit may be considered to be an apple if it is red, round, and about 3″ in diameter. A Naive Bayes classifier considers each of these “features” (red, round, 3” in diameter) to contribute independently to the probability that the fruit is an apple, regardless of any correlations between features.\n",
    "\n",
    "Features, however, aren’t always independent which is often seen as a shortcoming of the Naive Bayes algorithm and this is why it’s labeled “naive”.\n",
    "\n",
    "In a nutshell, the algorithm allows us to predict a class, given a set of features using probability. \n",
    "\n",
    "\n",
    "## Resources \n",
    "\n",
    "http://blog.aylien.com/naive-bayes-for-dummies-a-simple-explanation/\n",
    "\n",
    "https://www.youtube.com/watch?v=CPqOCI0ahss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male']\n"
     ]
    }
   ],
   "source": [
    "# Let's try Gaussian Naive Bayes \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb_model = gnb.fit(X,Y)\n",
    "\n",
    "gnb_prediction = gnb_model.predict([[190, 70, 43]])\n",
    "\n",
    "print(gnb_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors (KNN)\n",
    "\n",
    "An object is classified by a majority vote of its neighbors.\n",
    "\n",
    "KNN Algorithm is based on feature similarity: How closely out-of-sample features resemble our training set determines how we classify a given data point\n",
    "\n",
    "\n",
    "## Resources \n",
    "\n",
    "https://medium.com/@adi.bronshtein/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Building model; specifying how many neighbors should vote\n",
    "KNN_model = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "KNN_model = KNN_model.fit(X,Y)\n",
    "\n",
    "KNN_prediction = KNN_model.predict([[190, 70, 43]])\n",
    "\n",
    "print(KNN_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Prediction Accuracy\n",
    "\n",
    "<strong> Caution: </strong> I don't know if the following code is actually accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree is the best classifier.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "actual_gender =['male'] # This is the actual gender for the body measurements 190, 70, 43\n",
    "\n",
    "tree_accuracy = accuracy_score(actual_gender, prediction)\n",
    "log_accuracy = accuracy_score(actual_gender, log_prediction)\n",
    "gnb_accuracy = accuracy_score(actual_gender, gnb_prediction)\n",
    "KNN_accuracy = accuracy_score(actual_gender, KNN_prediction)\n",
    "\n",
    "\n",
    "classifiers =['Decision Tree', 'Logistic Regression', 'Gaussian Naive Bayes', 'KNN']\n",
    "accuracy = np.array([tree_accuracy, log_accuracy, gnb_accuracy, KNN_accuracy])\n",
    "max_accuracy = np.argmax(accuracy)\n",
    "print(classifiers[max_accuracy] + ' is the best classifier.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
